[{"id":3,"pagetitle":"Developer guide","title":"Developer Notes","ref":"/GraphNeuralNetworks.jl/graphneuralnetworks/dev/#Developer-Notes","content":" Developer Notes"},{"id":4,"pagetitle":"Developer guide","title":"Development Enviroment","ref":"/GraphNeuralNetworks.jl/graphneuralnetworks/dev/#Development-Enviroment","content":" Development Enviroment GraphNeuralNetworks.jl is package hosted in a monorepo that contains multiple packages.  The GraphNeuralNetworks.jl package depends on GNNGraphs.jl and GNNlib.jl, also hosted in the same monorepo. In order  pkg> activate .\n\npkg> dev ./GNNGraphs"},{"id":5,"pagetitle":"Developer guide","title":"Add a New Layer","ref":"/GraphNeuralNetworks.jl/graphneuralnetworks/dev/#Add-a-New-Layer","content":" Add a New Layer To add a new graph convolutional layer and make it available in both the Flux-based frontend (GraphNeuralNetworks.jl) and the Lux-based frontend (GNNLux), you need to: Add the functional version to GNNlib Add the stateful version to GraphNeuralNetworks Add the stateless version to GNNLux Add the layer to the table in docs/api/conv.md We suggest to start with implementing a self-contained Flux layer in GraphNeuralNetworks.jl, add the corresponding tests, and then when everything is working, move the implementation of the forward pass to GNNlib.jl. At this point, you can add the stateless version to GNNLux.jl. It could also be convenient to use the  @structdef  macro from  Autostruct.jl  to simultaneously generate the struct and the constructor for the layer. For example, the Flux implementation of  MEGNetConv  layer can be written as follows: using Flux, GraphNeuralNetworks, AutoStructs\n\n@structdef function MEGNetConv(ch::Pair{Int, Int}; aggr = mean)\n    nin, nout = ch\n    ϕe = Chain(Dense(3nin, nout, relu),\n               Dense(nout, nout))\n\n    ϕv = Chain(Dense(nin + nout, nout, relu),\n               Dense(nout, nout))\n\n    return MEGNetConv(ϕe, ϕv, aggr)\nend\n\nFlux.@layer MEGNetConv\n\nfunction (l::MEGNetConv)(g::AbstractGraph, x::AbstractMatrix, e::AbstractMatrix)\n    ē = apply_edges(g, xi = x, xj = x, e = e) do xi, xj, e\n        l.ϕe(vcat(xi, xj, e))\n    end\n    xᵉ = aggregate_neighbors(g, l.aggr, ē)\n    x̄ = l.ϕv(vcat(x, xᵉ))\n    return x̄, ē\nend"},{"id":6,"pagetitle":"Developer guide","title":"Versions and Tagging","ref":"/GraphNeuralNetworks.jl/graphneuralnetworks/dev/#Versions-and-Tagging","content":" Versions and Tagging Each PR should update the version number in the Porject.toml file of each involved package if needed by semnatic versioning. For instance, when adding new features GNNGraphs could move from \"1.17.5\" to \"1.18.0-DEV\". The \"DEV\" will be removed when the package is tagged and released. Pay also attention to updating the compat bounds, e.g. GraphNeuralNetworks might require a newer version of GNNGraphs."},{"id":7,"pagetitle":"Developer guide","title":"Generate Documentation Locally","ref":"/GraphNeuralNetworks.jl/graphneuralnetworks/dev/#Generate-Documentation-Locally","content":" Generate Documentation Locally For generating the documentation locally cd docs\njulia (@v1.10) pkg> activate .\n  Activating project at `~/.julia/dev/GraphNeuralNetworks/docs`\n\n(docs) pkg> dev ../ ../GNNGraphs/\n   Resolving package versions...\n  No Changes to `~/.julia/dev/GraphNeuralNetworks/docs/Project.toml`\n  No Changes to `~/.julia/dev/GraphNeuralNetworks/docs/Manifest.toml`\n\njulia> include(\"make.jl\")"},{"id":8,"pagetitle":"Developer guide","title":"Benchmarking","ref":"/GraphNeuralNetworks.jl/graphneuralnetworks/dev/#Benchmarking","content":" Benchmarking You can benchmark the effect on performance of your commits using the script  perf/perf.jl . First, checkout and benchmark the master branch: julia> include(\"perf.jl\")\n\njulia> df = run_benchmarks()\n\n# observe results\njulia> for g in groupby(df, :layer); println(g, \"\\n\"); end\n\njulia> @save \"perf_master_20210803_mymachine.jld2\" dfmaster=df Now checkout your branch and do the same: julia> df = run_benchmarks()\n\njulia> @save \"perf_pr_20210803_mymachine.jld2\" dfpr=df Finally, compare the results: julia> @load \"perf_master_20210803_mymachine.jld2\"\n\njulia> @load \"perf_pr_20210803_mymachine.jld2\"\n\njulia> compare(dfpr, dfmaster)"},{"id":9,"pagetitle":"Developer guide","title":"Caching tutorials","ref":"/GraphNeuralNetworks.jl/graphneuralnetworks/dev/#Caching-tutorials","content":" Caching tutorials Tutorials in GraphNeuralNetworks.jl are written in Pluto and rendered using  DemoCards.jl  and  PlutoStaticHTML.jl . Rendering a Pluto notebook is time and resource-consuming, especially in a CI environment. So we use the  caching functionality  provided by PlutoStaticHTML.jl to reduce CI time. If you are contributing a new tutorial or making changes to the existing notebook, generate the docs locally before committing/pushing. For caching to work, the cache environment(your local) and the documenter CI should have the same Julia version (e.g. \"v1.9.1\", also the patch number must match). So use the  documenter CI Julia version  for generating docs locally. julia --version # check julia version before generating docs\njulia --project=docs docs/make.jl Note: Use  juliaup  for easy switching of Julia versions. During the doc generation process, DemoCards.jl stores the cache notebooks in docs/pluto_output. So add any changes made in this folder in your git commit. Remember that every file in this folder is machine-generated and should not be edited manually. git add docs/pluto_output # add generated cache Check the  documenter CI logs  to ensure that it used the local cache:"}]